作者：dahang feng
链接：https://www.zhihu.com/question/48537863/answer/149339740
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

我的研究领域是麦克风阵列信号处理，从2013年开始做远场语音识别的信号处理部分，目前也有了一些经验，分享下我的看法，欢迎指正。我认为远场语音识别技术难点可以分为3个部分，第一个是多通道同步采集硬件研发，第二个是前端麦克风阵列降噪算法，第三个是后端语音识别与前端信号处理算法的匹配。首先多通道同步采集硬件是研究前端降噪算法的前提，只有先拿到一些麦克风阵列的数据，才能根据实际采集的数据进行算法的研发和调优。目前市面上主流的codec芯片最多支持4通道同步采集，这对于麦克风阵列来说是不够的，比如echo音响，采用了7个麦克风，再加上一个喇叭的参考信号，所以它至少需要8通道同步采集，即2个4通道的codec芯片。为了使两个codec芯片同步，需要一颗FPGA芯片来协助完成，同时麦克风还需要一些配套的模拟滤波放大电路，中间有很多都是经验性的东西，并且在echo以前，消费电子上很少有集成4个麦克风的情况，所以研究的人很少，这也增加了该硬件的研发难度。当然如果仅仅是为了研究可以直接购买一些多通道同步采集设备，节省硬件研发周期。其次是麦克风阵列降噪算法的研发。目前影响远场语音识别的难点是播放状态下打断，房间混响和非平稳噪声干扰等。播放状态下打断是指设备在播放音乐或tts的时候可以对它再次下达指令，这就需要回声消除技术，将设备自身播放的声音从麦克风接收到的信号除去，这个技术在手机上已经非常成熟了，比如上面@Kent Zhang提到的speex和webrtc的开源软件中都有该算法，但这两个开源软件为了达到更大的回声抑制效果，使用了大量的非线性处理手段，因此如果直接用在远场语音识别领域，效果并不好（我们已经做过测试）。在研究中我也发现，语音识别引擎对于语音信号的非线性处理非常敏感，简单的说就是语音失真少一些即使背景噪声有残留，也不会影响语音识别率。房间混响会造成麦克风接收到的信号有很长的拖尾，让人听起来感觉发闷，在实际中人耳具有自动解混响的能力，所以人在实际房间中相互交流并没有影响反而觉得声音饱满，但是这个对于语音识别来说是致命的，我觉得可能的原因是房间的冲击响应太长，一般有400ms-1000ms，而语音识别一帧的长度只有50ms，即使DNN有记忆能力，但也有限，所以在混响中语音识别率下降。远场语音识别以前，由于需求不大，对于去混响研究的不多，一般以倒谱平均、谱减法为主，但这类方法对远场语音识别率提升不大，目前比较好的去混响算法是日本NTT部门研究的多步线性预测方法，有兴趣的可以尝试一下。非平稳噪声干扰主要是利用波束形成去除，在做波束形成之前需要先知道说话人的方向，这就需要测向功能，即波达方向估计，学术上的论文一般研究如何提高测向的精度和分辨率，但这些指标在实际中意义不大，实际中更需要解决的是如何在混响的条件下提高波达方向估计的鲁棒性。知道方向之后，就可以做波束形成，抑制周围的非平稳噪声，由于消费产品价格和尺寸的限制，麦克风的个数间距有限，因此必须用自适应波束形成算法，简单说就是保护主方向说话人的同时，自动在噪声方向形成零点，对噪声进行抑制。经过测试，波束形成算法可以大幅提高语音识别率。最后就是语音识别引擎要和前端降噪算法进行匹配。目前的识别算法还是训练数据和测试数据越匹配效果越好。目前各家的语音识别引擎主要是利用手机上收集的语音进行训练的，因此只适用于近讲情况。同样道理，为了提升远场语音识别，就需要用远场的语音数据训练声学模型，而远场语音数据又太复杂（混响、噪声），这就需要信号处理的手段让数据尽量变的单一一些，最佳的方法就是利用麦克风阵列采集的信号经过前端降噪算法后的数据去训练语音识别引擎，效果应该会有大幅提升。此外，目前远场语音识别也分为两派，一派认为利用深度学习的办法也可以实现去混响降噪声的目的，另外一派是用麦克风阵列信号处理的方法去除混响和噪声，从目前的产品上看麦克风阵列信号处理的方式在实际中用的更多一些，echo用了7个麦克风，叮咚用了8个麦克风，google home也用了两个麦克风。个人觉得在远场语音识别这块，麦克风阵列信号处理还是一定需要的，因为人有这么聪明的大脑还长了两个耳朵，而目前DNN和人脑差别又很大，所以更需要多个麦克风做辅助，相信随着DNN的发展应该有一天可以只用两个麦克风就可以彻底解决远场语音识别问题，但这个时间估计还会很长。
